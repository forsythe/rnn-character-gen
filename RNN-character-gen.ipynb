{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 31784 chars, 48 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('trump.txt', 'r').read().lower()\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has {} chars, {} unique'.format(data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 0, 'n': 1, 'x': 2, '2': 3, '?': 4, '-': 5, '6': 6, 'h': 7, 'l': 8, '3': 9, 'u': 10, ',': 11, '.': 12, 'r': 13, '9': 14, 'i': 15, 'a': 16, 'm': 17, 'q': 18, '\\n': 19, '(': 20, 'p': 21, 'g': 22, ']': 23, 'v': 24, '7': 25, '$': 26, 'z': 27, 'b': 28, 'j': 29, 'f': 30, 'd': 31, '0': 32, 'e': 33, '5': 34, 's': 35, 'y': 36, '4': 37, '8': 38, 'c': 39, 'w': 40, '[': 41, ')': 42, 'k': 43, 't': 44, ':': 45, ' ': 46, '1': 47}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i, ch in enumerate(chars)}\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'o', 1: 'n', 2: 'x', 3: '2', 4: '?', 5: '-', 6: '6', 7: 'h', 8: 'l', 9: '3', 10: 'u', 11: ',', 12: '.', 13: 'r', 14: '9', 15: 'i', 16: 'a', 17: 'm', 18: 'q', 19: '\\n', 20: '(', 21: 'p', 22: 'g', 23: ']', 24: 'v', 25: '7', 26: '$', 27: 'z', 28: 'b', 29: 'j', 30: 'f', 31: 'd', 32: '0', 33: 'e', 34: '5', 35: 's', 36: 'y', 37: '4', 38: '8', 39: 'c', 40: 'w', 41: '[', 42: ')', 43: 'k', 44: 't', 45: ':', 46: ' ', 47: '1'}\n"
     ]
    }
   ],
   "source": [
    "ix_to_char = { i:ch for i,ch in enumerate(chars)}\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "vector_for_char_a = np.zeros((vocab_size, 1)) #r, c\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "hidden_size = 200\n",
    "seq_length = 25 #generate 25 chars at a time\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "#r, c: [0, 1]\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "* _Why_ are parameters to connect the hidden layer to the output\n",
    "* _bh_ contains the hidden bias\n",
    "* _by_ contains the output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss input:  \n",
    "* List of input chars\n",
    "* List of target chars\n",
    "* Previous hidden state\n",
    "\n",
    "Function output:\n",
    "* Loss\n",
    "* Gradient for each parameter between layers\n",
    "* The last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunc(inputs, targets, hprev):\n",
    "    #store our inputs, hidden states, outputs, and probability values\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    \n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    #forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1)) #81*1\n",
    "        xs[t][inputs[t]] = 1 #1 hot\n",
    "        \n",
    "                            # 100*81.81*1 + ... => 100*1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) #el wise tanh\n",
    "        \n",
    "                    # 81x100.100x1 => 81x1\n",
    "        ys[t] = np.dot(Why, hs[t]) + by #vector output, not num! 81*1\n",
    "        \n",
    "        #81x1\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) #vector/scalar division, softmax of predictions\n",
    "        \n",
    "        #cross entropy loss, calculates loss when the correct probability is 1\n",
    "        loss += -np.log(ps[t][targets[t],0]) \n",
    "        \n",
    "    #backward pass\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0]) #scalar\n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 \n",
    "        #the correct value now has \"negative\" gradient, so it should \"keep going\"\n",
    "        #the other values are literally their error, because they should be 0\n",
    "        \n",
    "        #a giant matrix of the \"errors\", opinions from each character \n",
    "        #on how each weight should change (more positive is higher \"error\")\n",
    "        #81x1.1x100 => 81x100\n",
    "        dWhy += np.dot(dy, hs[t].T) \n",
    "\n",
    "        #just use output error as derivative of output bias\n",
    "        dby += dy \n",
    "        \n",
    "        #backpropagate!\n",
    "                #81*100.100x1 =>81x1 + scalar\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h           \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                                                                \n",
    "        \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "      \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    sample a sequence of integers from the model                                                                                                                                                \n",
    "    h is memory state, seed_ix is seed letter for first time step   \n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    #create vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    #list to store generated chars\n",
    "    ixes = []\n",
    "    #for as many characters as we want to generate\n",
    "    for t in range(n):\n",
    "        #a hidden state at a given time step is a function \n",
    "        #of the input at the same time step modified by a weight matrix \n",
    "        #added to the hidden state of the previous time step \n",
    "        #multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(Why, h) + by\n",
    "        ## probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #pick one with the highest probability \n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel()) #p: probability dist\n",
    "        #create a vector\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[ix] = 1\n",
    "        #add it to the list\n",
    "        ixes.append(ix)\n",
    "\n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print('----\\n {} \\n----'.format(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " gr$qg9n-pl0wssp s]w\n",
      "oks??3,,)sl?um,zn.?vad[8\n",
      "9y, 53hdp?c\n",
      "u,w$y5ffpplx$srwj41by1ih.bjkpxc2,re]n6fg  cv5:k5vq:qmi2l(p,$qe72h9ecs-ia7iahmygewm(t14c8uleqlf bksqdu9i(u,kf)xlhsr]u)p)7w6o9[2[5 4jpsx]-t9]h)6y \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [8, 33, 35, 35, 46, 44, 7, 16, 1, 46, 0, 1, 33, 46, 36, 33, 16, 13, 46, 7, 16, 35, 46, 21, 16]\n",
      "targets [33, 35, 35, 46, 44, 7, 16, 1, 46, 0, 1, 33, 46, 36, 33, 16, 13, 46, 7, 16, 35, 46, 21, 16, 35]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print(\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print(\"targets\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 96.78001954118726\n",
      "----\n",
      " ]a,5az7i4v36:m83zejqk]i).8l)(m872\n",
      "j5i5v8q1:vjrd76c:xhocvg3wb .-lqovg3)surixw.(w,(rl42c5-chcqjq70afzsqk[aho170ayp.4oeh: :0fm)1s3b?3e7g20d\n",
      "sair, vaa2[9,w8sb[pkpno-dy9(g:2 l$hqfa8yq$ -u(l($e84vs856vry,]f \n",
      "----\n",
      "iter 1000, loss: 91.38001849732053\n",
      "----\n",
      " ge0ig,eii eme :ec    fp alee.tnesst-ta]ngmtaemtoe g,gytts2inp t y7 otataytadwarod evr,c dasneoghtdensmu ohternvo ak\n",
      "rsoni p grt cer,sdb wa\n",
      " k reiei tnirtasaciania a,lflt re  t ,7l xhtnu ln  ihemtnh i  \n",
      "----\n",
      "iter 2000, loss: 79.76089898248762\n",
      "----\n",
      " e uon he, thigrcnrnanm\n",
      "4e cnmt senf aomo o,s. kraping thet.\n",
      "doias il\n",
      "pedrlucathng an7te dy txu o, nceo,y o aisecmlow wicize 9oeg i7y$ asli yrukwho tinellnaehas ee,sbre alrag mern ahlors pten ,e ]hhay. \n",
      "----\n",
      "iter 3000, loss: 70.20436391871611\n",
      "----\n",
      " feceoven gory ogon, in sh ve shorec. ihese esd anl eos  obhry cave tongu  weod pomleud etas omingehcin ansried\n",
      "\n",
      "\n",
      ")ve agu pad nanve lle thehere mwhnl hind anr oecngouiusd toery amepousr on iss peresa p \n",
      "----\n",
      "iter 4000, loss: 64.98532684716069\n",
      "----\n",
      " . wh tuve phe caf cae pwe d core rpiiyd petres. dags, page lwer as lver tor panar lthed xan 1uxtirardsso othe fenthrnts pmeat onn futocsidryrte cblawsiou cerins olod fouof tac the t erpewher puandk  l \n",
      "----\n",
      "iter 5000, loss: 62.323609947355344\n",
      "----\n",
      " winzerilo ficre thattiar th bet b them opeldtend gapre cat hanw is.\n",
      "\n",
      "ehe oosinhe fames ost tove thc, thy yan ihghy t mheatoutmoettor ri, ax heorn peun in ist tor tistoset owd h rinedonher, fo0t we lhr \n",
      "----\n",
      "iter 6000, loss: 60.26801487807576\n",
      "----\n",
      " dsred sante. debemlapand whot, as to, odpisl are on gald sromrevac ame als oucre douly parry. on omale inaleg ul alet, an pivkaghe dathand thicint ars ard coererage thererled asge dhames,t, mantey ala \n",
      "----\n",
      "iter 7000, loss: 59.003589036168165\n",
      "----\n",
      " lytili sace an tand shapt woverams pas thi naple tit.\n",
      "\n",
      "unt tod toylycorinn ane hant and nor sy tradyt malits fave lomice. yuritlend send,, er tees to padl ga fan,. hin th tof perey pre b copuld budicn \n",
      "----\n",
      "iter 8000, loss: 58.0935082735897\n",
      "----\n",
      " lto sepposcominsd parsberit ate our ame the amiss palesinsarin il there tever the ant honpeinng thatt,, turyr we sand fslenwt afs outto tew to spid den5, aan hever. in wote. aur def, ha-le chal cerets \n",
      "----\n",
      "iter 9000, loss: 57.48334283090342\n",
      "----\n",
      " ossion dopedop dwint woire, as inkiviclan lsoidabiathy. clearlo bo.\n",
      "\n",
      "and tht., ame falmiringr ics, thonsme meidtas.\n",
      "his pay. is the nuexte, vertalys, af heicgour is sed aless thoro fond thithtaxte in  \n",
      "----\n",
      "iter 10000, loss: 57.08532045687485\n",
      "----\n",
      " we halpunn cante to as gorer tatin. ace couht abrentor eds, acsirite haspe-has yrican thaf, thalte ile thw preghind dike ede, and that peosath. roasin diede wars eny ald pereres thourh0 ant thattha po \n",
      "----\n",
      "iter 11000, loss: 56.1687837076545\n",
      "----\n",
      " anu-ls ldoter the coud ghne af soml0 gra5dere and tnigtcilor ong recpelkin sweassthe ero meampeng ofur and bigh, we es ies prit meeclor ofd andt. the himere be p1aloltor, tourim ceble pead safer cofr  \n",
      "----\n",
      "iter 12000, loss: 55.58387403684182\n",
      "----\n",
      " s the our encloprirs dgto we tacat, rawis, juther ly. ancenen whesitfs navo lummet emeqlobp pether, proxs ofrind and decjywersinc for ote lorts emidghe cate hirverartorg whit. sto the sucto seravretil \n",
      "----\n",
      "iter 13000, loss: 55.29927215316718\n",
      "----\n",
      " n. funy, ast that momiraing ghe sted abtirn ed of is paas ade iniby yalyi. weress thittoure acer te htrtising somto lourica tree ice cicice.\n",
      "uied anp noveven over gas ho faly wer us at q1\n",
      "p-fake to de \n",
      "----\n",
      "iter 14000, loss: 55.075281535904345\n",
      "----\n",
      "  lewd amt upres, te crote there to decalr ale to dhecke es rois heired day peighser at buces wolt, and nas sin yo n1cacy poaldeiz.\n",
      "\n",
      "200 bute the perind amicaver faviss candega prometis om ie worekin w \n",
      "----\n",
      "iter 15000, loss: 54.79549733249287\n",
      "----\n",
      "  y ale yib date avimite domes thesr.\n",
      "aple everedsull ,\n",
      "\n",
      "ihe ribled papists, at fall suthmtrand of rliniy m hesion calkescy youn, jous thats rant gate agefe palye strhatahied of fouthe cutwe jucler yit \n",
      "----\n",
      "iter 16000, loss: 54.15958413720447\n",
      "----\n",
      " or teple iresr on co alpeice aple amerdtiny to ane naverd rereed keragn harel wilk cob plase hstheslor the cesrbotarne sill gassie. on oun bey gon ure dhtardertots mo pci3eresed, thour sirthsth sherer \n",
      "----\n",
      "iter 17000, loss: 53.782300912628145\n",
      "----\n",
      " ered.\n",
      "\n",
      "bulighisand astry. now in weriont and yomitonid trengamind. wete and yo tofe thaluis the treft we benlyacos. stan tey oa prant.\n",
      "\n",
      "ing ho binginthl ha ducr wecane, age beyer thony tnin terking av \n",
      "----\n",
      "iter 18000, loss: 53.799722069591596\n",
      "----\n",
      " sa, ol our mald.\n",
      "\n",
      "ge wer ame the ne-wisu salllio nufd\n",
      "$vapreges. andw mime dade an avee ther 20s mive mumed thare oncots nonicia safryengict myiw sinen entag dalr tied if fure $20.\n",
      "\n",
      "nagecristang morec \n",
      "----\n",
      "iter 19000, loss: 53.87948367243145\n",
      "----\n",
      " ei covedths.\n",
      "\n",
      "ene ai hewe the toxcer, te thid. wtarist, tere in ther b tot ons icees nand af, ame mi4 whe asbure cand hared wors orear is the $kest ard and and haverioons ars it an pars ive, ig dontit \n",
      "----\n",
      "iter 20000, loss: 53.07854336840993\n",
      "----\n",
      " t thats ar toolmed in ard siirst robse deipleies amealy ag frankis wouss licallisgre, te thoutes ove tcouns ferind whe ir jucthe dorins. nuthe thiad olisghed ranryss, whiu nuaser juped telkiof wesiend \n",
      "----\n",
      "iter 21000, loss: 52.78920279879366\n",
      "----\n",
      "  werenn ro and foor and al ly dpumbe tremmtore to as ene ave verftor limigthintever whatg tn need ofrices stath bmomy an omo langrent wo touthor, aver. and buderm ape nyround perviengond thes lecinile \n",
      "----\n",
      "iter 22000, loss: 52.587475000744895\n",
      "----\n",
      " asd id as whors. to ive for. as sored con ove ther, ans we edeioen calder whs, in the cockingera.d wro thogt, stold imuwernigngring ond. thas thaor the muwt nind revice, jught aidtok of wase mucin tha \n",
      "----\n",
      "iter 23000, loss: 52.58957558960371\n",
      "----\n",
      " l fice to hhe7 centithtorlu and in the seriighninin andtilizent ans in copent merve.\n",
      "\n",
      "calllus fextonizin taniss and eroug a mily hserls terrisg whain. toud keryyas nock cale thin the.,\n",
      "\n",
      "wher me tre be \n",
      "----\n",
      "iter 24000, loss: 52.65801868543094\n",
      "----\n",
      " e rutre jestmes cow peounse ef fipperser we porsestare ard ast. in the the pintingonith, fraqian sitts gintous, ove . fouts sounst or hist cjins hare bus feres. to peot and ars fprory, thes sperte on  \n",
      "----\n",
      "iter 25000, loss: 51.98177020133771\n",
      "----\n",
      "  the und nogy alas et-ongantins mayh, cjesited catilt ivil got.\n",
      "\n",
      "ame our love toust, y, will ive wugen thoy whitl, ianal, whes. as save whis to ngiers. we lore to dicen, ame. hbigl. boul loricaly bris \n",
      "----\n",
      "iter 26000, loss: 51.63688649219297\n",
      "----\n",
      "  moqe. imave thig finst poapilotarl ball allitioh norp fith the hafd corung siy, tel, grang, conimess whe wirs i tryene. wes are of ly atinng ase wil toe dsuston. cemas thathes sires. for alynese or c \n",
      "----\n",
      "iter 27000, loss: 51.77325399255553\n",
      "----\n",
      " . ou mond trasty, trobp dutiny, ayn herins so sexner fareres and list the sate. thout, ad armpefs, oprasingse of our  hark amen treinmigh hatzing, fortode, we pire to ceseres, ray, womes theve onuasfr \n",
      "----\n",
      "iter 28000, loss: 51.69668035446195\n",
      "----\n",
      " ap wonsa lagian op werseredss we int thand peobind miletted soruranishe are we and thes sheis falib. gneares sto decillid6 is ho wern toses, to browt nat axind a sas a fpall otring nave adist in nean  \n",
      "----\n",
      "iter 29000, loss: 51.598405629487516\n",
      "----\n",
      " men of deungmie, punted hath dedon. buontoty as witor theal oc-hal. in wetters. of resemerd can: bemut he peeked bi-kavicherd of has thy belicall0 be\n",
      "bream, antour. themarnmingee, in the 20 toricaon t \n",
      "----\n",
      "iter 30000, loss: 51.0063636645943\n",
      "----\n",
      " e athed ame.\n",
      "\n",
      "names foreredthery, touy he pronciel fan we hore og amreddsiny and junden, walis soiveste vaciss annustion, our faprent and and pirts ole im als. and prrics to cooled we de$p6ricallyys s \n",
      "----\n",
      "iter 31000, loss: 50.87756775776355\n",
      "----\n",
      " ur and nimest in lamilla ote winh, rostredust bitere.\n",
      "\n",
      "ererencre the t ang reatitetalong mouvlistoricath a poave bomya oungreat pnowty of frost ghare to ameresster theabro:trefalreyshirment to , we wi \n",
      "----\n",
      "iter 32000, loss: 51.01706107574285\n",
      "----\n",
      "  broe the the are treiles thazage is somerel an, y walirous, to foars ale-hatl we bucu. glear getpat am(10. siverh and cot yogf, malrye ongrugs mrounis ane soy thererk with inear ax canlrogte ty deroi \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 33000, loss: 51.15521021375037\n",
      "----\n",
      " in streded aler andirto one tor whata of daded wheis wounting ind deicontonth, con.\n",
      "t: thatncins hhes un.\n",
      "\n",
      "pevucedha ponger ase coseris of thes, tand wille hak thegread the.\n",
      "\n",
      "a feve threiting anrorili \n",
      "----\n",
      "iter 34000, loss: 50.46723942602622\n",
      "----\n",
      "  pithta, nis yousyas the coull biculationizis and nget of it thirg pugre we tostry we we reand of.\n",
      "\n",
      "the aleldly perersententey froudbly the bnesto fus frucanser sos tor to b elk yi 250 ie.\n",
      ",0. anthati \n",
      "----\n",
      "iter 35000, loss: 50.170050675656036\n",
      "----\n",
      "  ogringrear, a peaington, treaved nosen sow st-buucsss, that beneme corming ine that caker toun younty reates angrectame ous thes. alicgen keverdtoun fore reave dus bese scraftonder mo chare, anth pro \n",
      "----\n",
      "iter 36000, loss: 50.16531574467243\n",
      "----\n",
      " ld taly state plongitith sy trengush farery of tanut we bepte.\n",
      "\n",
      "a fare saed sugrealls hm nome the to pefatsarn nable-orhe heap age thest necpringnst pot bes rucen sucling tivis we hoocracl to bemated, \n",
      "----\n",
      "iter 37000, loss: 50.16944861051427\n",
      "----\n",
      " er the iling y i ace of the ico.\n",
      "anith, intrsitay bes the ammurd poucldars fars mocled wererloves hat yound of are chazing ntange. counisy our sping cruss.y, sidenm hasd powiming, yiiss mumeicth reici \n",
      "----\n",
      "iter 38000, loss: 50.419618007353094\n",
      "----\n",
      " t, debin guthafor nomerto is wuthec firsa at is colatars azeap a expogeces a abprant,0 dsto kroe our srigrisniis theiperesthis mous.\n",
      "\n",
      "ove co siitiin the ducererraded bomestesonis vatt gerildar the eve \n",
      "----\n",
      "iter 39000, loss: 49.63768548748313\n",
      "----\n",
      " l strcest by whaaz be inho. ly to plome jusserung ana.\n",
      "\n",
      "ntaven, thes nese reath if fochir sutl has the ind. athens ris vilmens tomittot agero, we hade assugh finil fur teny who soalist on iva the cant \n",
      "----\n",
      "iter 40000, loss: 49.52527821210469\n",
      "----\n",
      " intcex. in in we.\n",
      "\n",
      "a cooplyonstan and and that.\n",
      "\n",
      "an is stoges, seclldetions worketh stoinosse. cois. ou salkit.\n",
      "\n",
      "in american thy. ammert and, wert tax. workef coldages wiansuro lils tonistaliine in om \n",
      "----\n",
      "iter 41000, loss: 49.549531765202644\n",
      "----\n",
      " esthe trating four, yof wrecam in therut yo mote telly apprest we ferntiung ard verig foud ame we. to pormofshhyo recosene hil hosrioghep my thouas stasd uncyint beathye, the noar of the thed goccong  \n",
      "----\n",
      "iter 42000, loss: 49.55988667775347\n",
      "----\n",
      " frexharp.\n",
      "\n",
      "athorch the balls ivilies.\n",
      "\n",
      "noty sommeds on reafiming desangrincina herorestrat ter . is thad ucall sancies same imeram usthid asorfo bar tinder. acproth. cterest, whsed ges warkerur efreve \n",
      "----\n",
      "iter 43000, loss: 49.50698151570359\n",
      "----\n",
      " ve canpthastar0 poopour in that he respirw aeve can, walle our roofrught wio slant is ander chef fyoingentesien letk tory, if he dettrily tho to the kregens serbea, an. mericj sprolss imve., whave cou \n",
      "----\n",
      "iter 44000, loss: 48.91819600876654\n",
      "----\n",
      "  buth roc, tho agetiis saprourdy oth werl, ie reve. befeans mefaton betire nring ane thane forly cavemry comgintorleds enpires one edroules.\n",
      "\n",
      "0heourhy till beis a proy, to mfiking lowtertry a conideny \n",
      "----\n",
      "iter 45000, loss: 48.87223063599466\n",
      "----\n",
      " e mart to and fey pomick allory hadenice arter. be wfier. berardngadveng andsit, yowisteris hour exleapled, htrorgrecares of cotorh fut to trenge thedring wall, ave axpered to peonsen atn am1,006, tha \n",
      "----\n",
      "iter 46000, loss: 49.08548089845561\n",
      "----\n",
      " ay in nuth and a have sudem, an tho nutherinuttle foruscoll and tho for the netde ass jure beinisha sleid forming sithey your the tos neny. far the jydiwi farerasis hove the tongand a reopes basrede p \n",
      "----\n",
      "iter 47000, loss: 49.167087952095045\n",
      "----\n",
      " regex stouns hass bone of mmugtonhribe, ay. cals greices and will, atilest ays, to potrest brigh and that the detcin the rant, and hart. weriagedy aritre mout, a detiystent te we sonth sicrioe pacts,  \n",
      "----\n",
      "iter 48000, loss: 48.647241333345086\n",
      "----\n",
      " lans deat iprored been and stiite ins peo denountred nuw coll thim expoures galle on vied. freapleses of id anfarsibly co mpear alate ase pirel amerses. gons thes htit on at one mugre chailily we ur u \n",
      "----\n",
      "iter 49000, loss: 48.364328271302334\n",
      "----\n",
      "  creave his vis one every in whas pat, tile ane our bet ur paphtreve treand watmor on they forryek nomple ane the dades, ambestro our nating owages wat, and thineths wit toope our diveremy tay, cagne  \n",
      "----\n",
      "iter 50000, loss: 48.25410419492224\n",
      "----\n",
      " ariod sand cans tory to condary to dewamied work pruced of is fard. is cilitisg beree moy ack gtansuve not beuplomottsition. and be curseds um thee, to hou the priironds fooclimer in theming jold to w \n",
      "----\n",
      "iter 51000, loss: 48.47078898936314\n",
      "----\n",
      " hasighez our whors gesiarg.\n",
      "\n",
      "alear, we us loopfont thesse hate, tomene com im whanl are and there to unt idstiplalar.\n",
      "\n",
      "we allaly. we harh pete amerours ivea, frominight.\n",
      "\n",
      "expencals nest they unedreice \n",
      "----\n",
      "iter 52000, loss: 48.733995988721254\n",
      "----\n",
      "  co resertaytar, o ca a salith edrat and is as ras. our we dris wilcauns ask stot sonmeston ton, lecally that dacens fredin the has bint sustine in ctiont moitans. acl to of nit echially tho whit ilto \n",
      "----\n",
      "iter 53000, loss: 49.036879532226386\n",
      "----\n",
      " ho to will ty sere comisy : we moret, we be dereweer, homes.\n",
      "an arking be ureny inataran thask agreash gresterp, a yoris, irse these to nearsth stand. becareds, tork this chear. keally bissure talled  \n",
      "----\n",
      "iter 54000, loss: 48.637417907443925\n",
      "----\n",
      " r ando butilice leis infare mompuriling can to gat our, no mave amading pilligh. dyiit urealde. oul oftoud amercenicon syard tho cong-yoy and and reslaly, berageaces.s\n",
      "\n",
      "couls.\n",
      "\n",
      "farors, muppell we ane  \n",
      "----\n",
      "iter 55000, loss: 48.15197757141105\n",
      "----\n",
      " thoond.\n",
      "\n",
      "od nfane infrouse hear. waing willoun the we blation sueshinusty, verned we isfoote steisyand to ark, il pave with on the reapsing geveret tou wan thapket.\n",
      "\n",
      "of. tame nucpempibrork 1 t0arst of \n",
      "----\n",
      "iter 56000, loss: 48.14333296214274\n",
      "----\n",
      " nd is we fer inecem.\n",
      "\n",
      "that the will any in of covertorin we americht, of ficeatilizoctan, ter pow the sthat moterer hast, nave aristhork, wat the wij als. imicn ald yous, that buen friss dothad and pr \n",
      "----\n",
      "iter 57000, loss: 48.06572310694077\n",
      "----\n",
      " poobsy we arst ot as thenuton to peof wofar hid eradinisound riganstall fligessar basef whop the reiresh tresy rilltery, bicars our indtre 1pe cully the be are wewict tererk ot poely rotren witha and  \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0   \n",
    "while True: #n<=1000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFunc(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print('iter {}, loss: {}'.format(n, smooth_loss)) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "\n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_length # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
